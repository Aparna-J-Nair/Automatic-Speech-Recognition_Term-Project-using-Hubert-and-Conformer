# Automatic-Speech-Recognition_Term-Project-using-Hubert-and-Conformer

## HuBERT (Hidden unit BERT) 
A new approach for learning self-supervised speech representations, to help to model these types of rich lexical and non-lexical information in audio.
HuBERT model either matches or improves upon the state-of the-art wav2vec 2.0 performance on the Librispeech (960h).It shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.It's simplicity and stability will help natural language processing and speech researchers to more broadly adopt learned discrete representations in their work.The quality of HuBERT’s learned presentations facilitates easy deployment to many different downstreams speech applications. 

## Conformer: Convolution-augmented Transformer for Speech Recognition
The objective is to perform automatic speech recognition using conformer which is a combination of convolution neural networks and transformer to fetch local and global dependencies respectively of an audio sequence. On the widely used Libri Speech benchmark, the model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test-clean/test-other

## Dataset
LibriSpeech is a corpus of approximately 1000 hours of 16khz read English speech. The audio is in English.
The whole corpus is divided into train, test and validation
Dataset used in this project is test-clean

